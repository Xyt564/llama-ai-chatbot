# ğŸ¤– Llama-3.1-8B Local CLI Chatbot

A fast, fully local CLI chatbot powered by **Meta Llama-3.1-8B-Instruct** using `llama-cpp-python`.

This project is optimized for low-to-mid range hardware and provides a smooth streaming chat experience directly in your terminal â€” no APIs, no cloud, fully offline.

---

## âœ¨ Features

- âš¡ Optimized for 8GB RAM systems  
- ğŸ’¬ Real-time streaming responses  
- ğŸ§  Persistent chat history (JSON)  
- ğŸ“Š Built-in chat statistics  
- ğŸ—‚ï¸ Save and manage multiple conversations  
- ğŸ–¥ï¸ Clean and simple CLI interface  
- ğŸ”’ Fully local inference (no internet required after setup)  
- ğŸ‡¬ğŸ‡§ Timezone-aware chat timestamps (Europe/London)

---

## ğŸ§ª Tested Hardware

**This project has ONLY been tested on the following system:**

- **CPU:** Intel i3-1215U  
- **RAM:** 8GB  
- **GPU:** Intel UHD Graphics  
- **OS:** Linux (Pop!_OS / Debian-based)

> âš ï¸ Performance and memory usage may vary on other hardware.

---

## â— Important â€” Model Not Included

The AI model is **NOT included in this repository** because GitHub has file size limits.

You must download the model manually from Hugging Face.

---

## ğŸ“¥ Required Model

Download the GGUF model from:

ğŸ‘‰ https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF

**Required file:**

```
Llama-3.1-8B-Instruct-Q4_K_M.gguf

```

---

## ğŸ“ Project Structure

After setup, your project should look like:

```

.
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Llama-3.1-8B-Instruct-Q4_K_M.gguf
â”œâ”€â”€ chat_history/
â””â”€â”€ assets/

````

---

## ğŸš€ Installation

### 1. Clone the repository

```bash
git clone https://github.com/Xyt564/llama-ai-chatbot.git
cd llama-ai-chatbot
````

---

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

### 3. Create the models directory

```bash
mkdir -p models
```

> Save ur Ai model in this models folder

---

### 4. Download and place the model

Move the downloaded file to:

```
models/Llama-3.1-8B-Instruct-Q4_K_M.gguf
```

âš ï¸ The program will fail to start if the model is not in this exact location.

---

### 5. Run the chatbot

```bash
python3 main.py
```

On first launch, the model will load into memory â€” this may take a little time on lower-end CPUs.

---

## ğŸ’¬ Available Commands

Inside the chatbot, you can use:

| Command        | Description                |
| -------------- | -------------------------- |
| `/help`        | Show help menu             |
| `/clear`       | Clear current chat history |
| `/save [name]` | Save conversation          |
| `/list`        | List saved chats           |
| `/stats`       | Show chat statistics       |
| `/history [n]` | Show recent messages       |
| `/quit`        | Exit the program           |

---

## ğŸ“¸ Screenshot / Demo:

![Chatbot Running](assets/demo.png)

---

## âš™ï¸ Performance Notes

This configuration is tuned specifically for **8GB RAM systems**: 

* Context length: **24k tokens**
* Quantization: **Q4_K_M**
* CPU threads: **7**
* KV cache: ~6.5GB
* Model size: ~4.5GB

> can be made less but it will very slowly. visit my phi-2 gui ai chatbot for lower ram systems

If you experience:

* âŒ Out of memory
* âŒ Heavy swapping
* âŒ Slow responses

You may need to:

* reduce `n_ctx`
* reduce threads
* use a smaller quant

---

## ğŸ› ï¸ Requirements

* Python **3.10+**
* Linux recommended
* ~7GB free RAM minimum
* GGUF model file

---

## ğŸ“Œ Notes (Only do this if you know what ur doing)

* Runs CPU-only by default
* GPU acceleration can be enabled by adjusting `n_gpu_layers`
* Chat history is stored in `chat_history/`
* Streaming is enabled for better responsiveness

---

## â­ Acknowledgements

* Meta â€” Llama models
* llama.cpp
* llama-cpp-python

```
