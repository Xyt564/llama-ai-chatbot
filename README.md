# ü§ñ Llama-3.1-8B Local CLI Chatbot

A fast, fully local CLI chatbot powered by **Meta Llama-3.1-8B-Instruct** using `llama-cpp-python`.

This project is optimized for low-to-mid range hardware and provides a smooth streaming chat experience directly in your terminal ‚Äî no APIs, no cloud, fully offline.

---

## ‚ú® Features

- ‚ö° Optimized for 8GB RAM systems  
- üí¨ Real-time streaming responses  
- üß† Persistent chat history (JSON)  
- üìä Built-in chat statistics  
- üóÇÔ∏è Save and manage multiple conversations  
- üñ•Ô∏è Clean and simple CLI interface  
- üîí Fully local inference (no internet required after setup)  
- üá¨üáß Timezone-aware chat timestamps (Europe/London)

---

## üß™ Tested Hardware

**This project has ONLY been tested on the following system:**

- **CPU:** Intel i3-1215U  
- **RAM:** 8GB  
- **GPU:** Intel UHD Graphics  
- **OS:** Linux (Pop!_OS / Debian-based)

> ‚ö†Ô∏è Performance and memory usage may vary on other hardware.

---

## ‚ùó Important ‚Äî Model Not Included

The AI model is **NOT included in this repository** because GitHub has file size limits.

You must download the model manually from Hugging Face.

---

## üì• Required Model

Download the GGUF model from:

üëâ https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF

**Required file:**

```
Llama-3.1-8B-Instruct-Q4_K_M.gguf
```

---

## üìÅ Project Structure

After setup, your project should look like:

```

.
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ Llama-3.1-8B-Instruct-Q4_K_M.gguf
‚îú‚îÄ‚îÄ chat_history/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ security.md
‚îú‚îÄ‚îÄ assets/
|   ‚îî‚îÄ‚îÄ demo.png
‚îî‚îÄ‚îÄ License

````

---

## üöÄ Installation

### 1. Clone the repository

```bash
git clone https://github.com/Xyt564/llama-ai-chatbot.git
```

```bash
cd llama-ai-chatbot
```

---

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

### 3. Create the models directory

```bash
mkdir -p models
```

> Save ur Ai model inside the models folder

---

### 4. Download and place the model

Move the downloaded file to:

```
models/Llama-3.1-8B-Instruct-Q4_K_M.gguf
```

‚ö†Ô∏è The program will fail to start if the model is not in this exact location.

---

### 5. Run the chatbot

```bash
python3 main.py
```

> On first launch, the model will load into memory ‚Äî this may take a little time on lower-end CPUs.

---

## üí¨ Available Commands

Inside the chatbot, you can use:

| Command        | Description                |
| -------------- | -------------------------- |
| `/help`        | Show help menu             |
| `/clear`       | Clear current chat history |
| `/save [name]` | Save conversation          |
| `/list`        | List saved chats           |
| `/stats`       | Show chat statistics       |
| `/history [n]` | Show recent messages       |
| `/quit`        | Exit the program           |

---

## üì∏ Screenshot / Demo:

![Chatbot Running](assets/demo.png)

---

## ‚öôÔ∏è Performance Notes

This configuration is tuned specifically for **8GB RAM systems**: 

* Context length: **24k tokens**
* Quantization: **Q4_K_M**
* CPU threads: **7**
* KV cache: ~6.5GB
* Model size: ~4.5GB

> Ram usage can be lowered but it affect the performance heavily and make it extremly slow.

> Visit my phi-2 gui ai chatbot for lower ram systems.

If you experience:

* ‚ùå Out of memory
* ‚ùå Heavy swapping
* ‚ùå Slow responses

You may need to:

* reduce `n_ctx`
* reduce threads
* use a smaller quant

---

## üõ†Ô∏è Requirements

* Python **3.10+**
* Linux recommended
* ~7GB free RAM minimum
* GGUF model file

---

## üìå Notes

* Runs CPU-only by default
* GPU acceleration can be enabled by adjusting `n_gpu_layers`
* Chat history is stored in `chat_history/`
* Streaming is enabled for better responsiveness

> Only change these if you know what ur doing

---

## ‚≠ê Acknowledgements

* Meta ‚Äî Llama models
* llama.cpp
* llama-cpp-python

```
