# ğŸ” Security Policy

## ğŸ“Œ Supported Versions

This project is currently maintained on the latest commit of the main branch.

|Version | Supported |
|--------|-----------|
| Latest | âœ…        |
----------------------

---

## ğŸš¨ Reporting a Vulnerability

If you discover a security vulnerability, **please report it responsibly and privately**.

### Preferred method

- Open a **request** on GitHub  
- Contact the maintainer directly via GitHub

---

## ğŸ“ What to Include

To help triage quickly, include:

- Clear description of the vulnerability  
- Steps to reproduce  
- Potential impact  
- Proof of concept (if possible)  
- Your environment details

---

## â±ï¸ Response Expectations

- Initial response: typically within **3â€“7 days**  
- Fix timeline: depends on severity and complexity  
- Security patches will be released as soon as reasonably possible

(This is a solo-maintained project, so timelines are best-effort.)

---

## ğŸ›¡ï¸ Security Notes

This project:

- Runs fully **locally**
- Does **not** transmit data externally
- Does **not** include telemetry
- Stores chat history locally in JSON

### âš ï¸ User Responsibility

Users should:

- Only download models from trusted sources (e.g., Hugging Face)  
- Avoid running untrusted GGUF files  
- Keep their Python environment updated  
- Review code before running in sensitive environments  

---

## ğŸ”’ Scope

This security policy applies to:

- The CLI application
- Chat history handling
- Local model loading

It does **not** cover:

- Third-party model files  
- llama.cpp internals  
- llama-cpp-python upstream issues  
- User system misconfiguration  

---

## ğŸ™ Responsible Disclosure

Please give reasonable time for a fix before public disclosure.

Thank you for helping keep the project secure â¤ï¸

---
